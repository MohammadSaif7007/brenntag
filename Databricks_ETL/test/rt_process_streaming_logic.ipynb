{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7884d1-fb78-49a9-88fd-8c1efee6fc30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/a11081990@outlook.com/brenntag/streaming_eventhub_to_hive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c315c9-d0d2-46ab-85b9-5031f3ad9502",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.dlsassign.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.dlsassign.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.dlsassign.dfs.core.windows.net\", \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2023-08-31T03:26:51Z&st=2023-08-07T19:26:51Z&spr=https&sig=wU3ruQRItsr%2BNfdVcUerP4mqT2WHtA5QYOrPWZEGPwA%3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9326007a-18c5-43fd-9c6a-c9af19e7abbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n----------------------------------------------------------------------\nRan 1 test in 0.069s\n\nOK\n\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest.mock import patch, Mock\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class TestStreamingJob(unittest.TestCase):\n",
    "    @patch(\"pyspark.sql.DataFrameReader.load\")\n",
    "    def test_process_streaming_logic(self, mock_load):\n",
    "        # Mock the readStream.load method to return a DataFrame\n",
    "        mock_load.return_value = Mock(spec=DataFrame)\n",
    "        # Call the process_streaming_logic function\n",
    "        df_stream = process_streaming_logic()\n",
    "        # Example: Check if the returned value is of the expected type\n",
    "        self.assertTrue(isinstance(df_stream, DataFrame))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_runner = unittest.main(argv=[''], exit=False)\n",
    "    test_runner.result.printErrors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2510339b-80ae-49f8-b21e-888b2d0d43e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3585601948732443,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "rt_process_streaming_logic",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
