{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7646fb0-cd1c-49be-9aff-3663916711e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/a11081990@outlook.com/brenntag/CustomerDataPrep\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d375c35f-5f32-411b-ac76-eb31baf510d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.dlsassign.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.dlsassign.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.dlsassign.dfs.core.windows.net\", \"sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2023-08-31T03:26:51Z&st=2023-08-07T19:26:51Z&spr=https&sig=wU3ruQRItsr%2BNfdVcUerP4mqT2WHtA5QYOrPWZEGPwA%3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55887b12-e360-4f46-a472-d1153932c7b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n|         customer_id|company_name|\n+--------------------+------------+\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n|06ec6a8a-c3d9-4b0...|     Skilith|\n+--------------------+------------+\n\n+------------+--------------------+----------------------+\n|company_name|         customer_id|specialized_industries|\n+------------+--------------------+----------------------+\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n|     Skilith|06ec6a8a-c3d9-4b0...|               Colours|\n+------------+--------------------+----------------------+\n\nTotal rows before removing null values: 1\nTotal rows after removing null values: 10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n----------------------------------------------------------------------\nRan 1 test in 2.039s\n\nOK\n\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest.mock import patch, Mock\n",
    "from pyspark.sql import Row\n",
    "#from your_script_name import process_data  # Import your script's function\n",
    "\n",
    "source_data = 'abfs://demo-data@dlsassign.dfs.core.windows.net/Customers.csv'\n",
    "existing_data = 'abfs://raw@dlsassign.dfs.core.windows.net/customers.parquet'\n",
    "\n",
    "# data_transfer(source_data,existing_data)\n",
    "class TestRegression(unittest.TestCase):\n",
    "    \"\"\"\n",
    "        Test the process_data function by mocking DataFrame methods.\n",
    "        \n",
    "        This test verifies the behavior of the process_data function by creating mock DataFrames\n",
    "        to simulate the behavior of read.parquet and filter methods. The process_data function is\n",
    "        then called, and its result is compared with expected outcomes.\n",
    "        \n",
    "        This test ensures that the process_data function performs the necessary DataFrame\n",
    "        transformations correctly and produces the expected DataFrame result.\n",
    "    \"\"\"\n",
    "\n",
    "    @patch(\"pyspark.sql.DataFrameReader.parquet\")\n",
    "    @patch(\"pyspark.sql.DataFrame.filter\")\n",
    "    def test_process_data(self, mock_filter, mock_parquet):\n",
    "        # Mock the parquet read and DataFrame methods\n",
    "        mock_parquet.return_value = self.create_mock_parquet()\n",
    "        mock_filter.return_value = self.create_mock_filtered_df()\n",
    "\n",
    "        # Call the process_data function\n",
    "        result_df = process_data()\n",
    "\n",
    "        # Perform assertions based on your expectations\n",
    "        expected_row_count = 10  # Adjust the expected count value\n",
    "        self.assertEqual(result_df.count(), expected_row_count)\n",
    "        # Add more assertions as needed\n",
    "\n",
    "    def create_mock_parquet(self):\n",
    "        # Mocked DataFrame for read.parquet\n",
    "        data = [\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"}\n",
    "            # Add more test data dictionaries as needed\n",
    "        ]\n",
    "        return spark.createDataFrame(data)\n",
    "\n",
    "    def create_mock_filtered_df(self):\n",
    "        # Mocked DataFrame for filter\n",
    "        data = [\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"},\n",
    "            {\"customer_id\": \"06ec6a8a-c3d9-4b06-bab2-fd4af45f0788\", \"company_name\": \"Skilith\", \"specialized_industries\": \"Colours\"}\n",
    "            \n",
    "        ]\n",
    "        return spark.createDataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_runner = unittest.main(argv=[''], exit=False)\n",
    "    test_runner.result.printErrors()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "rt_cleansed_customer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
